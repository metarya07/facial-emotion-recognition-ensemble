# -*- coding: utf-8 -*-
"""sentimental.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UEV3TmWY2E1nZeM0C3SaMvf40PAhinnr
"""

!pip install --upgrade pip



!pip install mediapipe





import cv2, math,numpy as np,mediapipe as mp,torch
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image
from google.colab.output import eval_js
from IPython.display import Javascript, clear_output, Image as IPImage, display
from base64 import b64decode
from pathlib import Path

torch.set_grad_enabled(False)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE  = torch.float16 if DEVICE=="cuda" else torch.float32
print("Device:", DEVICE, "dtype:", DTYPE)

EMOTIONS = ["Angry","Disgust","Fear","Happy","Sad","Surprise","Neutral"]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  LOAD models ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MODEL_IDS = [
    "motheecreator/vit-Facial-Expression-Recognition",
    "trpakov/vit-face-expression",
    "dima806/facial_emotions_image_detection"
]

processors, models = [], []
for mid in MODEL_IDS:
    print("‚Üí loading", mid)
    proc = AutoImageProcessor.from_pretrained(mid)
    mdl  = AutoModelForImageClassification.from_pretrained(
             mid, torch_dtype=DTYPE).to(DEVICE).eval()
    if mdl.config.num_labels != 7:
        print("   ‚ö†  skipped ‚Äì has", mdl.config.num_labels, "labels")
        continue
    processors.append(proc); models.append(mdl)
print("‚úÖ using", len(models), "models in the ensemble.")

processors, models = [], []
for m in MODEL_IDS:
    print("Loading", m)
    proc = AutoImageProcessor.from_pretrained(m)
    mdl  = AutoModelForImageClassification.from_pretrained(
              m, torch_dtype=DTYPE).to(DEVICE).eval()
    processors.append(proc); models.append(mdl)
print("Loaded", len(models), "models.")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  detector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
detector = mp.solutions.face_detection.FaceDetection(
              model_selection=0, min_detection_confidence=0.55)

def detect_align(bgr):
    h,w = bgr.shape[:2]
    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
    res = detector.process(rgb)
    if not res.detections: return None, None
    det = max(res.detections,
              key=lambda d: d.location_data.relative_bounding_box.width *
                            d.location_data.relative_bounding_box.height)
    box = det.location_data.relative_bounding_box
    x1,y1 = int(box.xmin*w), int(box.ymin*h)
    x2,y2 = int((box.xmin+box.width)*w), int((box.ymin+box.height)*h)
    face  = bgr[max(0,y1):y2, max(0,x1):x2].copy()

    # align using eye line
    kp = det.location_data.relative_keypoints
    rx,ry = int(kp[0].x*w)-x1, int(kp[0].y*h)-y1
    lx,ly = int(kp[1].x*w)-x1, int(kp[1].y*h)-y1
    ang   = math.degrees(math.atan2(ly-ry, lx-rx))
    M = cv2.getRotationMatrix2D(((x2-x1)//2,(y2-y1)//2), ang, 1.0)
    face = cv2.warpAffine(face, M, face.shape[1::-1])
    return Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB)), (x1,y1,x2,y2)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  predict ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def predict(pil):
    logits = torch.zeros(len(EMOTIONS), device=DEVICE)
    for proc, mdl in zip(processors, models):
        inp = proc(images=pil, return_tensors="pt").to(DEVICE, DTYPE)
        logits += torch.softmax(mdl(**inp).logits.squeeze().float(), -1)
    probs = (logits / len(models)).cpu().numpy()
    idx   = int(np.argmax(probs))
    return EMOTIONS[idx], float(probs[idx]), probs

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  webcam ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
JS = Javascript(r"""
async function snap(){
  const div=document.createElement('div'),v=document.createElement('video');
  v.style.display='block'; v.width=400;
  const btn=document.createElement('button');btn.textContent='üì∏ Capture';
  div.append(v,btn);document.body.append(div);
  const str=await navigator.mediaDevices.getUserMedia({video:true});
  v.srcObject=str; await v.play();
  google.colab.output.setIframeHeight(document.documentElement.scrollHeight,true);
  await new Promise(r=>btn.onclick=r);
  const c=document.createElement('canvas');
  c.width=v.videoWidth;c.height=v.videoHeight;
  c.getContext('2d').drawImage(v,0,0);
  str.getTracks()[0].stop();div.remove();
  return c.toDataURL('image/jpeg',0.9);
}""")

def capture(path="frame.jpg"):
    display(JS)
    data = eval_js("snap()")
    Path(path).write_bytes(b64decode(data.split(',')[1]))
    return path
#--------------------------helper------------------------------------------

import csv, time, pandas as pd, torch.nn as nn, torch.optim as optim

DATA_CSV = "labels.csv"
EMB_CACHE = "embeddings.pt"        # saved tensor of (N, 768) features
LABEL_CACHE = "labels.pt"          # saved tensor of (N,) ints
BATCH_RETRAIN = 20                 # retrain head after this many new samples

# ‚ø°  linear head on top of one backbone (use the first ViT for embeddings)
BACKBONE = models[0]               # choose any model
EMB_DIM  = BACKBONE.config.hidden_size
head     = nn.Linear(EMB_DIM, 7).to(DEVICE)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(head.parameters(), lr=1e-4)

def get_embedding(pil):
    with torch.no_grad():
        x = processors[0](images=pil, return_tensors="pt").to(DEVICE, DTYPE)
        hidden = BACKBONE.vit(**x, output_hidden_states=True).hidden_states[-1]
        # CLS token
        return hidden[:,0,:].float()

def save_label(row):
    new_file = not Path(DATA_CSV).exists()
    with open(DATA_CSV, "a", newline="") as f:
        w = csv.writer(f)
        if new_file:
            w.writerow(["time","img","pred","true"])
        w.writerow(row)

def maybe_retrain():
    # count how many samples since last retrain
    df = pd.read_csv(DATA_CSV)
    if len(df) % BATCH_RETRAIN != 0: return
    print(f"üîÑ  Retraining head on {len(df)} samples ‚Ä¶")
    X = torch.load(EMB_CACHE) if Path(EMB_CACHE).exists() else torch.empty(0, EMB_DIM)
    y = torch.load(LABEL_CACHE) if Path(LABEL_CACHE).exists() else torch.empty(0, dtype=torch.long)

    # load latest BATCH_RETRAIN entries
    new = df.tail(BATCH_RETRAIN)
    new_emb = torch.vstack([get_embedding(Image.open(p)) for p in new.img]).cpu()
    new_lbl = torch.tensor([EMOTIONS.index(l) for l in new.true])
    torch.save(torch.cat([X, new_emb]), EMB_CACHE)
    torch.save(torch.cat([y, new_lbl]), LABEL_CACHE)

    # simple one‚Äëepoch fine‚Äëtune
    head.train()
    optimizer.zero_grad()
    out = head(torch.cat([X, new_emb]).to(DEVICE))
    loss = criterion(out, torch.cat([y, new_lbl]).to(DEVICE))
    loss.backward(); optimizer.step()
    head.eval()
    print("‚úÖ  Head updated ‚Äì loss {:.4f}".format(loss.item()))

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  run  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_with_feedback():
    while True:
        clear_output(wait=True)
        img = cv2.imread(capture())
        if img is None: continue
        face, box = detect_align(img)
        if face is None:
            print("No face."); continue

        # 1. ensemble prediction
        lbl, conf, probs = predict(face)

        # 2. ask user to confirm / correct
        print(f"Predicted: {lbl}  (confidence {conf:.2f})")
        inp = input(f"Press Enter to confirm, or type true label [{', '.join(EMOTIONS)}]: ").strip()
        true_lbl = inp if inp in EMOTIONS else lbl  # default = accept

        # 3. log
        ts   = int(time.time())
        path = f"capt_{ts}.jpg"
        cv2.imwrite(path, img)
        save_label([ts, path, lbl, true_lbl])

        # 4. (optional) retrain linear head every BATCH_RETRAIN samples
        maybe_retrain()

        # 5. draw and show
        x1,y1,x2,y2 = box
        cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)
        cv2.putText(img,f"{true_lbl}",(x1,y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX,0.9,(0,255,0),2)
        cv2.imwrite("out.jpg", img); display(IPImage("out.jpg"))

        for e,p in zip(EMOTIONS,probs):
            print(f"{e:<8}: {p*100:.1f}%")
        print("\n(Enter q to quit)")
        if input("> ").lower().strip() == "q":
            break


run_with_feedback()



# ======================= charts  ========================
# 1. Running accuracy  ‚Ä¢  2. True‚Äëlabel counts  ‚Ä¢  3. Confusion matrix
# 4. Percentage of each emotion detected
import pandas as pd, matplotlib.pyplot as plt, matplotlib.dates as mdates
from datetime import datetime
from pathlib import Path

CSV = "labels.csv"          # file created by run_with_feedback()

if not Path(CSV).exists():
    print("‚ùó  labels.csv not found ‚Äì capture & label some frames first.")
else:
    df = pd.read_csv(CSV)
    if df.empty:
        print("üõà  labels.csv is empty ‚Äì run_with_feedback() to add data.")
    else:
        # --- common prep ----------------------------------------------------
        df["time"] = pd.to_datetime(df["time"], unit="s")
        df["correct"] = (df["pred"] == df["true"]).astype(int)

        # --- 1. Running accuracy -------------------------------------------
        plt.figure(figsize=(10, 3))
        plt.plot(df["time"], df["correct"].expanding().mean(), marker="o")
        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%H:%M:%S"))
        plt.title("Running accuracy of the ensemble")
        plt.ylabel("accuracy"); plt.ylim(0, 1); plt.grid(True)
        plt.tight_layout(); plt.show()

        # --- 2. True‚Äëlabel counts ------------------------------------------
        counts = df["true"].value_counts().sort_index()
        plt.figure(figsize=(8, 3))
        counts.plot(kind="bar")
        plt.title("True‚Äëlabel distribution"); plt.ylabel("frequency")
        plt.xticks(rotation=45); plt.tight_layout(); plt.show()

        # --- 3. Confusion matrix -------------------------------------------
        cm = pd.crosstab(df["true"], df["pred"]).reindex(index=counts.index,
                                                         columns=counts.index,
                                                         fill_value=0)
        plt.figure(figsize=(6, 5))
        plt.imshow(cm, cmap="Blues"); plt.colorbar(label="count")
        plt.xticks(range(len(cm.columns)), cm.columns, rotation=45)
        plt.yticks(range(len(cm.index)), cm.index)
        for i in range(len(cm.index)):
            for j in range(len(cm.columns)):
                plt.text(j, i, cm.iloc[i, j], ha='center', va='center')
        plt.title("Confusion matrix (true vs. predicted)")
        plt.tight_layout(); plt.show()

        # --- 4. Percentage per emotion -------------------------------------
        pct = (counts / counts.sum() * 100).sort_values(ascending=False)
        plt.figure(figsize=(8, 4))
        pct.plot(kind="bar")
        plt.title("Percentage of each emotion detected")
        plt.ylabel("% of samples"); plt.ylim(0, 100)
        plt.xticks(rotation=45); plt.tight_layout();plt.show()

import mediapipe
print(mediapipe)
print(mediapipe.__file__)
print(dir(mediapipe)[:20])

import mediapipe as mp

import mediapipe
from mediapipe.python.solutions import face_detection
from mediapipe.python.solutions import drawing_utils

!pip uninstall -y mediapipe mediapipe-rpi mediapipe-silicon

!pip install mediapipe==0.10.13 --no-cache-dir

import mediapipe as mp

print("MediaPipe version:", mp.__version__)
print("mp file:", mp.__file__)
print("Has solutions:", hasattr(mp, "solutions"))
print("Solutions keys:", dir(mp.solutions)[:5])

import mediapipe as mp

detector = mp.solutions.face_detection.FaceDetection(
    model_selection=0,
    min_detection_confidence=0.55
)